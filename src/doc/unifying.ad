= Virtual Graphs 
[.etp]
https://github.com/stardog-union/stardog-docs/edit/develop/src/doc/unifying.ad[Edit This Section]&nbsp;image:e1.png[Edit,16,16]
++++
<div style="clear: both;"></div>
++++

Stardog is an Enterprise Knowledge Graph platform, which means that it's also a
data unification platform. Enterprises have lots of data and lots of data
sources and almost all of them are locked away in IT silos and stovepipes that
impede insight, analysis, reporting, compliance, and operations.

State of the art IT management tells us to *organize data, systems, assets,
staffs, schedules, and budgets* vertically to mirror lines of business. But
increasingly all the internal and external demands on IT are *horizontal* in
nature: the data is organized vertically, but the enterprise increasingly needs
to access and understand it horizontally.

== Structured Data 

Stardog supports a set of techniques for unifing *structured enterprise data*,
chiefly, *Virtual Graphs*: tabular data declaratively mapped into a Stardog
database graph and queries by Stardog _in situ_, typically using SQL. Stardog
rewrites (a portion of) SPARQL queries against the Stardog database into SQL,
issues that SQL query to an RDBMS, and translates the SQL results into SPARQL
results. Virtual Graphs can be used for mapping any tabular data, e.g. CSV, to
RDF and Stardog will support mapping other tabular formats to graphs in future
releases, including XML, JSON, and enterprise directory services.

A Virtual Graph has three components:

* a unique name
* configuration options for the data source
* mappings for the data

A virtual graph's name must conform to the regular expression
`[A-Za-z]{1}[A-Za-z0-9_-]`. The configuration options include several
parameters, primarily JDBC connection parameters. Finally, the mappings define
how the tabular data stored in the RDBMS will be represented in
RDF. The mappings are defined using the http://www.w3.org/TR/r2rml/[R2RML] 
mapping language, but a simpler <<Stardog Mapping Syntax>> is also supported for
serializing R2RML mappings.

=== Supported RDBMSes
//See https://github.com/ontop/ontop/wiki/ObdalibPluginJDBC#Driver_class

Stardog Virtual Graphs supports the following relational database systems; please <<mailto:inquiries@complexible.com,inquire>> if you need support for another.

* PostgreSQL
* MySQL
* Microsoft SQL Server
* Oracle
* DB2
* H2
* SAP HANA

=== Managing Virtual Graphs

In order to query a Virtual Graph it first must be registered with Stardog.
Adding a new virtual graph is done via the following command:

[source,bash]
----
$ stardog-admin virtual add dept.properties dept.ttl
----

The first argument (`dept.properties`) is the configuration file for the virtual
graph and the second argument ('dept.ttl') is the mappings file. The name of the
configuration file is used as the name of the virtual graph, so the above
command registers a virtual graph named `dept`. Configuration files should be in
the Java properties file format and provide JDBC connection parameters. A
minimal example looks like this:

[source,bash]
----
jdbc.url=jdbc\:mysql\://localhost/dept
jdbc.username=admin
jdbc.password=admin
jdbc.driver=com.mysql.jdbc.Driver
----

NOTE: Stardog does not ship with JDBC drivers. You need to manually copy the JAR
file containing the driver to the `STARDOG/server/dbms/` directory so that it
will be available to the Stardog server. Server needs to be restarted after the
JAR is copied.

The credentials for the JDBC connection need to be provided in plain text. An
alternative way to provide credentials is to use the <<Using a Password File,
password file>> mechanism. The credentials should be stored in a password file
called `services.sdpass` located in `STARDOG_HOME` directory. The password file
entries are in the format `hostname:port:database:username:password` so for the
above example there should be an entry `localhost:*:dept:admin:admin` in this
file. Then the credentials in the configuration file can be omitted.

The configuration file can also contain a property called `base` to specify a
http://www.w3.org/TR/r2rml/#dfn-base-iri[base URI] for resolving relative URIs
generated by the mappings (if any). If no value is provided, the base URI will
be `virtual://myGraph` where `myGraph` is the name of the virtual graph.

The add command by default assumes the mappings are using the <<Stardog Mapping Syntax>>. Mappings in standard R2RML syntax can be used by adding the 
`--format r2rml` option in the above command.
If there are no mappings provided to the add commands then the 
http://www.w3.org/TR/rdb-direct-mapping/[R2RML direct mapping] is used. 

Registered virtual graphs can be listed:

[source,bash]
----
$ stardog-admin virtual list
1 virtual graphs
dept
----

The commands `virtual mappings` and `virtual options` can be used to retrieve
the mappings and configuration options associated with a virtual graph 
respectively. Registered virtual graphs can be removed using the `virtual remove`
command. See the <<Man Pages>> for the details of these commands.

=== Querying Virtual Graphs

Querying Virtual Graphs is done by using the `GRAPH` clause, using a special
graph URI in the form `virtual://myGraph` to query the Virtual Graph named
`myGraph`. The following example shows how to query `dept`:

[source,sparql]
----
SELECT * {
   GRAPH <virtual://dept> {
      ?person a emp:Employee ;
           emp:name "SMITH"
   }
}
----

Virtual graphs are defined globally in Stardog Server. Once they are registered
with the server they can be accessed via any Stardog database as allowed by the
<<Virtual Graph Security,access rules>>.

We can query the Stardog database and pull data from a virtual graph in a single
query. Suppose we have the `dept` Virtual Graph, defined as above, that contains
employee and department information, and the Stardog database contains data
about the interests of people. We can use the following query to combine the
information from both sources:

[source,sparql]
----
SELECT * {
   GRAPH <virtual://dept> {
      ?person a emp:Employee ;
           emp:name "SMITH"
   }
   ?person foaf:interest ?interest
}
----

NOTE: Query performance will be best if the `GRAPH` clause for Virtual Graphs is
as selective as possible.

=== Virtual Graph Security

Accessing Virtual Graphs can be controlled similar to regular named graphs as
explained in the <<Named Graph Security>> section. If named graph security is
not enabled for a database, all registered Virtual Graphs in the server will 
be accessible through that database. If named graph security is enabled for a 
database, then users will be able to query only the Virtual Graphs for which
they have been granted access. 

If the virtual graphs contain any sensitive information, then it is recommended
to enable named graph security globally by setting `security.named.graphs=true`
in `stardog.properties`. Otherwise creating a new database without proper
configuration would allow users to access those Virtual Graphs.

=== Materializing Virtual Graphs

In some cases you need to *materialize the information stored in RDBMS directly
into RDF*. There is a special command `virtual import` that can be used to
import the contents of the RDBMS into Stardog. The command can be used as
follows:

[source,bash]
----
$ stardog-admin virtual import myDb dept.properties dept.ttl
----

This command adds all the mapped triples from the RDBMS into the default graph.
Similar to `virtual add`, this command assumes <<Stardog Mapping Syntax>> by
default and can accept R2RML mappings using the `--format r2rml` option.

It is also possible to specify a target named graph by using the
`-g`/`--namedGraph` option:
	
[source,bash]
----
$ stardog-admin virtual import -g http://example.com/targetGraph myDb dept.properties dept.ttl
----

This `virtual import` command is equivalent to the following SPARQL update query:

[source,sparql]
----
ADD <virtual://dept> TO <http://example.com/targetGraph>
----

If the RDBMS contents change over time, and we need to update the materialization results 
in the future, we can clear the named graph contents and rematerialize again. This
can be done by using the `--remove-all` option in `virtual import` or with the following
SPARQL query:

[source,sparql]
----
COPY <virtual://dept> TO <http://example.com/targetGraph>
----

Query performance over materialized graphs will be
better as the data will be indexed locally by Stardog, but materialization may
not be practical in cases where frequency of change is very high.

=== CSV as Virtual Graph

Mappings can be used to treat CSV files as Virtual Graphs since they represent
tabular data similar to RDBMS tables. But unlike RDBMS tables, CSV files are
supported only for importing into Stardog. The same import command above can be
used to specify a mappings file and an input CSV file:

[source,bash]
----
$ stardog-admin virtual import myDB cars.ttl cars.csv
----

If the input file is using different kind of separators, e.g. tab character, a
configuration file can be provided

[source,bash]
----
$ stardog-admin virtual import myDB cars.properties cars.ttl cars.tsv
----

The configuration file for CSV files can specify values for the following
properties: `csv.separator` (character for separating fields), `csv.quote`
(character for strings), `csv.escape` (character for escaping special
characters), `csv.header` (boolean value for specifying whether or not the input
file has a header line at the beginning). Note that, whitespace characters in
Java properties file need to be escaped so if you want to import tab-separated
value files set `csv.separator=\t` in the configuration file.

=== Stardog Mapping Syntax

The Stardog Mapping Syntax (SMS) is an alternative way to serialize R2RML
mappings that is much simpler to read and write and has the same expressive
power as R2RML. Mappings written in SMS can be converted to R2RML and vice
versa. We will use the
http://www.w3.org/TR/r2rml/#example-input-database[example database] from the
R2RML specification to explain SMS. The SQL schema that corresponds to this
example is 

[source,sql]
----
CREATE TABLE "DEPT" (
      "deptno" INTEGER UNIQUE,
      "dname" VARCHAR(30),
      "loc" VARCHAR(100));
INSERT INTO "DEPT" ("deptno", "dname", "loc") 
       VALUES (10, 'APPSERVER', 'NEW YORK');

CREATE TABLE "EMP" (
      "empno" INTEGER PRIMARY KEY,
      "ename" VARCHAR(100),
      "job" VARCHAR(30),
      "deptno" INTEGER REFERENCES "DEPT" ("deptno"),
      "etype" VARCHAR(30));
INSERT INTO "EMP" ("empno", "ename", "job", "deptno", "etype" ) 
       VALUES (7369, 'SMITH', 'CLERK', 10, 'PART_TIME');
----

Suppose we would like to represent this information in RDF using the
same http://www.w3.org/TR/r2rml/#example-translationtable[translation for job
codes] as in the original example:

[source,bash]
----
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix emp: <http://example.com/emp/> .
@prefix dept: <http://example.com/dept/> .

dept:10 a dept:Department ;
    dept:location "NEW YORK" ;
    dept:deptno "10"^^xsd:integer .

emp:7369 a emp:Employee ;
    emp:name "SMITH" ;
    emp:role emp:general-office ;
    emp:department dept:10 .
----

SMS looks very similar to the output RDF representation:

[source,sparql]
----
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix emp: <http://example.com/emp/> .
@prefix dept: <http://example.com/dept/> .
@prefix sm: <tag:stardog:api:mapping:> .

dept:{"deptno"} a dept:Department ;
    dept:location "{\"loc\"}" ;
    dept:deptno "{\"deptno\"}"^^xsd:integer ;
    sm:map [
      sm:table "DEPT" ;
    ] .

emp:{"empno"} a emp:Employee ;
    emp:name "{\"ename\"}" ;
    emp:role emp:{ROLE} ;
    emp:department dept:{"deptno"} ;
    sm:map [
      sm:query """
        SELECT \"empno\", \"ename\", \"deptno\", (CASE \"job\"
            WHEN 'CLERK' THEN 'general-office'
            WHEN 'NIGHTGUARD' THEN 'security'
            WHEN 'ENGINEER' THEN 'engineering'
        END) AS ROLE FROM \"EMP\"
        """ ;
    ] .
----

SMS is based on Turtle, but it's not valid Turtle since it uses the
http://www.w3.org/TR/r2rml/#from-template[URI templates] of R2RML--curly braces
can appear in URIs. Other than this difference, we can treat an SMS document as
a set of RDF triples. SMS documents use the special namespace
`tag:stardog:api:mapping:` that we will represent with the `sm` prefix below.

Every subject in the SMS document that has a `sm:map` property maps a 
single row from the corresponding table/view to one or more triples. If an 
existing  table/view is being mapped, `sm:table` is used to refer to the table. 
Alternatively, a SQL query can be provided inline using the `sm:query` property.

The values generated will be a URI, blank node, or a literal based on the type
of the value used in the mapping. The column names referenced between curly
braces will be replaced with the corresponding values from the matching row.

SMS can be translated to the standard R2RML syntax. For completeness, we provide
the R2RML mappings corresponding to the above example:

[source,sparql]
----
@prefix rr: <http://www.w3.org/ns/r2rml#> .
@prefix emp: <http://example.com/emp#> .
@prefix dept: <http://example.com/dept#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@base <http://example.com/base/> .

<DeptTriplesMap>
    a rr:TriplesMap;
    rr:logicalTable [ rr:tableName "DEPT" ];
    rr:subjectMap [ rr:template "http://data.example.com/dept/{\"deptno\"}" ;
                    rr:class dept:Department ];
    rr:predicateObjectMap [
      rr:predicate	  dept:deptno ; 
      rr:objectMap    [ rr:column "\"deptno\""; rr:datatype xsd:positiveInteger ]
    ];
    rr:predicateObjectMap [
      rr:predicate	dept:location ; 
      rr:objectMap	[ rr:column "\"loc\"" ]
    ].

<EmpTriplesMap>
	a rr:TriplesMap;
    rr:logicalTable [ rr:sqlQuery """
        SELECT "EMP".*, (CASE "job"
            WHEN 'CLERK' THEN 'general-office'
            WHEN 'NIGHTGUARD' THEN 'security'
            WHEN 'ENGINEER' THEN 'engineering'
        END) AS ROLE FROM "EMP"
        """ ];
    rr:subjectMap [
        rr:template "http://data.example.com/employee/{\"empno\"}";
        rr:class emp:Employee
    ];    
    rr:predicateObjectMap [ 
      rr:predicate		emp:name ; 
      rr:objectMap    [ rr:column "\"ename\"" ]; 
    ];
    rr:predicateObjectMap [
        rr:predicate emp:role;
        rr:objectMap [ rr:template "http://data.example.com/roles/{ROLE}" ];
    ];    
    rr:predicateObjectMap [
        rr:predicate emp:department;
        rr:objectMap [ rr:template "http://example.com/dept/{\"deptno\"}"; ];
    ].
----


== Unstructured Data

Unifying unstructured data is, by necessity, a different process from unifying
structured or semistructured data. As of 4.2, Stardog includes a document
storage subsystem called BITESfootnote:[Blob Indexing and Text Enrichment with
Semantics], which provides configurable storage and processing for unifying
unstructured data with the Stardog graph.

=== Background & Assumptions

BITES makes some simplfying assumptions in Stardog {version} with respect to
unstructured data unification--

* Unbounded blobs in the database are a bad idea; Stardog manages documents (any
  files, actually) in the local filesystem, in AWS S3, or (soon) in HDFS.
* Documents are the most common kind of enterprise unstructured data.
* There is no general purpose natural language processing, which means 
  *extraction of structured data from unstructured documents* isn't a generally
  solved problem, which in turns means Stardog users have to define (an
  optional) workflow for Stardog to call on document ingest.
* We can't train a classifier on data we don't have; which, again, means
  Stardog users have to train classifiers, extractors, entity or relationship
  learners, etc. on their data. Stardog will unify the results into its graph.
* Sometimes document context matters, sometimes it doesn't; in other words, Stardog
  (optionally) store and retrieve the original document on demand over REST, but
  in some cases this isn't required or needed.

=== Basic Usage

See the <<Man Pages>> for the CLI usage details, including how to interact with
the HTTP API for BITES.

=== Storage

BITES allows storage and retrieval of documents in the form of files. Stardog
treats documents as opaque blobs of data; it defers to the extraction process to
make sense of individual documents. Document storage is independent of file and
data formats.

Stardog internally stores documents as files. The location of these files
defaults to a subdirectory of `STARDOG_HOME` but this can be overridden.
Documents can be stored on local filesystem, or an abstraction thereof,
accessible from the Stardog server or on Amazon S3 by setting the
`docs.filesystem.uri` configuration option. The exact location is given by the
`docs.path` configuration option.

=== Structured Data Extraction

BITES supports an optional processing stage in which a document is processed to
extract an RDF graph to add to the database. The default extractor, based on
Apache Tika, collects metadata about the document and asserts this set of RDF
statements to a named graph specific to the document. This pipeline is flexible
and allows for arbitrary extraction components including NLP and ML methods.

=== Text Extraction

The document store is fully integrated with Stardog's <<Full-Text Search>>. As
with RDF extraction, text extraction supports arbitrary file formats and
pluggable extractors are able to retrieve the textual contents of a document for
indexing. Once a document is added to BITES, its contents can be searched in the
same way as other literals using the standard `textMatch` predicate in SPARQL
queries.

=== Managing Documents

CRUD operations on documents can be performed from the command line,
Java API or HTTP API. Please refer to the
link:/java/snarl/com/complexible/stardog/docs/StardocsConnection.html[StardocsConnection]
API for details of using the document store from Java.

The following is an example session showing how to manage documents
from the command line:

[source,bash]
----
# We have a document stored in the file `whyfp90.pdf' which we will add to the document store
$ ls -al whyfp90.pdf
-rw-r--r-- 1 user user 200007 Aug 30 09:46 whyfp90.pdf

# We add it to the document store and receive the document's IRI as a return value
$ bin/stardog doc put myDB whyfp90.pdf
Successfully put document in the document store: tag:stardog:api:docs:myDB:whyfp90.pdf

# Alternatively, we can add it with a different name. Repeated calls
# will update the document and refresh extraction results
$ bin/stardog doc put myDB --name why-functional-programming-matters.pdf whyfp90.pdf
Successfully put document in the document store: tag:stardog:api:docs:myDB:why-functional-programming-matters.pdf

# We can subsequently retrieve documents and store them locally
$ bin/stardog doc get myDB whyfp90.pdf
Wrote document 'whyfp90.pdf' to file 'whyfp90.pdf'
# Local files will not be overwritten
$ bin/stardog doc get myDB whyfp90.pdf
File 'whyfp90.pdf' already exists. You must remove it or specify a different filename.

# How many documents are in the document store?
$ bin/stardog doc count myDB
Count: 2 documents

# Removing a document will also clear it's named graph and full-text search index entries
$ bin/stardog doc delete myDB whyfp90.pdf
Successfully executed deletion.
----

=== Named Graphs and Document Queries

Documents in BITES are identified by IRI. As shown in the command line examples
above, the IRI is returned from a document `put` call. The IRI is a combination
of a prefix, the database name, and the document name. The CLI uses the document
name to refer to the documents. The RDF index, and therefore SPARQL queries, use
the IRIs to refer to the documents. RDF assertions extracted from a document are
placed into a named graph identified by the document's IRI.

Here we can see the results of querying a document's named graph when using the
default metadata extractor:

[source,bash]
----
$ bin/stardog query execute myDB "select ?p ?o { graph <tag:stardog:api:docs:myDB:whyfp90.pdf> { ?s ?p ?o } }"

+--------------------------------------------+--------------------------------------+
|                     p                      |                  o                   |
+--------------------------------------------+--------------------------------------+
| rdf:type                                   | http://xmlns.com/foaf/0.1/Document   |
| rdf:type                                   | tag:stardog:api:docs:Document        |
| tag:stardog:api:docs:fileSize              | 200007                               |
| http://purl.org/dc/elements/1.1/identifier | "whyfp90.pdf"                        |
| rdfs:label                                 | "whyfp90.pdf"                        |
| http://ns.adobe.com/pdf/1.3/PDFVersion     | "1.3"                                |
| http://ns.adobe.com/xap/1.0/CreatorTool    | "TeX"                                |
| http://ns.adobe.com/xap/1.0/t/pg/NPages    | 23                                   |
| http://purl.org/dc/terms/created           | "2006-05-19T13:42:00Z"^^xsd:dateTime |
| http://purl.org/dc/elements/1.1/format     | "application/pdf; version=1.3"       |
| http://ns.adobe.com/pdf/1.3/encrypted      | "false"                              |
+--------------------------------------------+--------------------------------------+

Query returned 11 results in 00:00:00.045
----

=== Custom Extractors

The included metadata extractor is intentionally basic, especially when compared
to machine learning or text mining algorithms. A custom extractor connects the
document store to algorithms tailored specifically to your data. The extractor
SPI allows integration of any arbitrary workflow or algorithm from NLP methods
like part-of-speech tagging, entity recognition, relationship learning, or
sentiment analysis to machine learning models such as document ranking and
clustering.

Extracted RDF assertions are stored in a named graph specific to the document,
allowing provenance tracking and versatile querying. The extractor must
implement the
link:/java/snarl/com/complexible/stardog/docs/extraction/RDFExtractor.html[RDFExtractor]
interface. The convenience class
link:/java/snarl/com/complexible/stardog/docs/extraction/tika/TextProvidingRDFExtractor.html[TextProvidingRDFExtractor]
is provided which extracts the text from the document before calling the
extractor.

The text extractor SPI gives you the opportunity to support arbitrary
document formats. Implementations will be given a raw document and be
expected to extract a string of text which will be added to the
full-text search index. Text extractors should implement the
link:/java/snarl/com/complexible/stardog/docs/extraction/TextExtractor.html[TextExtractor]
interface.

Custom extractors are registered with the Java ServiceLoader under the
link:/java/snarl/com/complexible/stardog/docs/extraction/RDFExtractor.html[RDFExtractor]
or
link:/java/snarl/com/complexible/stardog/docs/extraction/TextExtractor.html[TextExtractor]
class names. Custom extractors can be referred to from the command
line or APIs by their fully qualified or "simple" class names.
